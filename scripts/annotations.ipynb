{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coupled-block",
   "metadata": {},
   "source": [
    "### 0. Instruction\n",
    "This file instructs you to create annotation file in the following format\n",
    "\n",
    "filename: annotations.npy\n",
    "\n",
    "shape: (number of sequences, number of region types, sequence length)\n",
    "\n",
    "        where the default number of region types is five, and the default sequence length is 101\n",
    "        Indicies correspond to each region type as: 0=5'UTR, 1=3'UTR, 2=exon, 3=intron, 4=CDS\n",
    "\n",
    "As long as your resulting file has the above format and corresponding dev.tsv file, analysis can be conducted without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-personal",
   "metadata": {},
   "source": [
    "### 1. Create fasta files for RBPs from the benchmark file\n",
    "The files generated using generate_datasets.py are in tsv formats. This codes here will generate those files in the fasta format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "velvet-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('./motif')\n",
    "from motif_utils import seq2kmer, kmer2seq\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "described-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the benchmark file is\n",
    "BENCHMARK_PATH = \"THIS_IS_THE_PATH_TO_YOUR_BENCHMARK_FILE\"\n",
    "OUTPUT_PATH = \"./datasets\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--path_to_benchmark\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to the benchmark file\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--path_to_output\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to the output directory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--kmer\",\n",
    "    default=3,\n",
    "    type=int,\n",
    "    required=False,\n",
    "    help=\"kmer of the output file\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_num\",\n",
    "    default=15000,\n",
    "    type=int,\n",
    "    required=False,\n",
    "    help=\"maximum number of samples to retrieve\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--test_ratio\",\n",
    "    default=0.2,\n",
    "    type=float,\n",
    "    required=False,\n",
    "    help=\"ratio of test data\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--random_seed\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    required=False,\n",
    "    help=\"seed number for random sampling\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args(args = [\"--path_to_benchmark\", BENCHMARK_PATH,\\\n",
    "                                                 \"--path_to_output\", OUTPUT_PATH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = 'original.fasta'\n",
    "\n",
    "def createfasta(args):\n",
    "    with open(args.input_file) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    filename = 'EMPTY'\n",
    "    i = 0\n",
    "    rbplist = []\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        if 'train_dir' in line:\n",
    "            pattern = '([A-Z0-9]+.negative)|([A-Z0-9]+.positive)'\n",
    "            filename = re.sub('[.]', '_', re.search(pattern, line).group())\n",
    "            # print(filename)\n",
    "            value = 0\n",
    "            if 'positive' in filename:\n",
    "                value = 1\n",
    "            filename = re.search('[A-Z0-9]+', filename).group()\n",
    "            filepath = os.path.join(args.output_dir, filename, OUTPUT_FILE)\n",
    "            \n",
    "            \n",
    "            if os.path.exists(filepath):\n",
    "                \n",
    "                if filename in rbplist:\n",
    "                    with open(filepath, mode='a') as f:\n",
    "                        flg = 0\n",
    "                        while flg == 0:\n",
    "                            found_header = re.findall(\">chr[0-9XYM]+:[0-9]+\\-[0-9]+\\([\\+\\-]\\)\", lines[i])\n",
    "                            found_sequence = re.findall(\"[AUGCTN]+\", lines[i])\n",
    "                            if len(found_header)==1:\n",
    "                                f.write(found_header[0] + \" rbp_bound: \" + str(value) + '\\n')\n",
    "                            elif len(found_sequence)==1:\n",
    "                                f.write(re.sub('U', 'T', found_sequence[0]) + '\\n')\n",
    "                            else:\n",
    "                                if i<len(lines):\n",
    "                                    print(\"ERROR: rbp {} line #{}\".format(filename, i))\n",
    "                                    print(lines[i])\n",
    "                            i += 1\n",
    "                            if i >= len(lines):\n",
    "                                break\n",
    "                            elif 'train_dir' in lines[i]:\n",
    "                                break\n",
    "                else:\n",
    "                    with open(filepath, mode='w') as f:\n",
    "                        flg = 0\n",
    "                        while flg == 0:\n",
    "                            found_header = re.findall(\">chr[0-9XYM]+:[0-9]+\\-[0-9]+\\([\\+\\-]\\)\", lines[i])\n",
    "                            found_sequence = re.findall(\"[AUGCTN]+\", lines[i])\n",
    "                            if len(found_header)==1:\n",
    "                                f.write(found_header[0] + \" rbp_bound: \" + str(value) + '\\n')\n",
    "                            elif len(found_sequence)==1:\n",
    "                                f.write(re.sub('U', 'T', found_sequence[0]) + '\\n')\n",
    "                            else:\n",
    "                                #found_nnn = re.findall(\"[N]+\", lines[i])\n",
    "                                #if found_nnn:\n",
    "                                #    print(\"found NNN: rbp {} line #{}\".format(filename, i))\n",
    "                                if i<len(lines):\n",
    "                                    print(\"ERROR: rbp {} line #{}\".format(filename, i))\n",
    "                                    print(lines[i])\n",
    "                            i += 1\n",
    "                            if i >= len(lines):\n",
    "                                break\n",
    "                            elif 'train_dir' in lines[i]:\n",
    "                                break\n",
    "                \n",
    "                rbplist.append(filename)\n",
    "\n",
    "            else:\n",
    "                print(\"ERROR\")\n",
    "        else:\n",
    "            print('ERROR')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-generic",
   "metadata": {},
   "source": [
    "### 2. Create bed files from original.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_DIR = \"./datasets\"\n",
    "FASTA_NAME = \"original.fasta\"\n",
    "BED_OUTPUT_DIR = \"\"\n",
    "BED_NAME = \"original.bed\"\n",
    "\n",
    "RBPS = (\"AARS\",)\n",
    "\n",
    "for rbp in RBPS:\n",
    "    fasta_file = os.path.join(MASTER_DIR, rbp, FASTA_NAME)\n",
    "    if len(BED_OUTPUT_DIR)==0:\n",
    "        BED_OUTPUT_DIR = os.path.join(MASTER_DIR, rbp)\n",
    "    bed_file = os.path.join(BED_OUTPUT_DIR, BED_NAME)\n",
    "\n",
    "    lines_bed = []\n",
    "\n",
    "    count = 0\n",
    "    with open(fasta_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if \">chr\" in line:\n",
    "                    #lines_bed.append(line[:-])\n",
    "                    found = re.findall(\">chr[0-9XYM]+:[0-9]+\\-[0-9]+\", line)\n",
    "                    if not len(found)==1:\n",
    "                        print('ERROR: found {} match at line #{}'.format(len(found), count))\n",
    "                        print(line)\n",
    "                    else:\n",
    "                        lines_bed.append(found[0][1:])\n",
    "                count += 1\n",
    "\n",
    "    with open(bed_file, 'w') as f:\n",
    "            f.write(\"\\n\".join(lines_bed))\n",
    "\n",
    "    # print(count)\n",
    "    # print(len(lines_bed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-glossary",
   "metadata": {},
   "source": [
    "### 3. Convert bed(hg19) to bed(hg38)\n",
    "3-1. upload the generated bed file to the UCSC liftover page (https://genome.ucsc.edu/cgi-bin/hgLiftOver)\n",
    "\n",
    "3-2. convert the version from \"Feb. 2009 (GRCh37/hg19)\" to \"Dec. 2013 (GRCh38/hg38)\" by setting the parameters as follows:\n",
    "\n",
    "    Minimum ratio of bases that must remap: 1\n",
    "    \n",
    "3-3. submit your bed file\n",
    "\n",
    "3-4. download the converted file and rename it as \"original_hg38.bed\"\n",
    "\n",
    "3-5. browse the failed list of sequences and copy and paste them into the file named \"fail.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-plymouth",
   "metadata": {},
   "source": [
    "### 4. Create original_hg38.fasta\n",
    "Create fasta file with hg38 versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_DIR = \"./datasets\"\n",
    "FASTA_NAME = \"original.fasta\"\n",
    "FASTA_OUTPUT_NAME = \"original_hg38.fasta\"\n",
    "FASTA_FAIL_NAME = \"fail.fasta\"\n",
    "BED_OUTPUT_DIR = \"\"\n",
    "BED_NAME = \"original_hg38.bed\"\n",
    "\n",
    "RBPS = (\"AARS\",)\n",
    "\n",
    "for rbp in RBPS:\n",
    "    fasta_file = os.path.join(MASTER_DIR, rbp, FASTA_NAME)\n",
    "    fasta_out_file = os.path.join(MASTER_DIR, rbp, FASTA_OUTPUT_NAME)\n",
    "    fasta_fail = os.path.join(MASTER_DIR, rbp, FASTA_FAIL_NAME)\n",
    "    if len(BED_OUTPUT_DIR)==0:\n",
    "        BED_OUTPUT_DIR = os.path.join(MASTER_DIR, rbp)\n",
    "    bed_file = os.path.join(BED_OUTPUT_DIR, BED_NAME)\n",
    "\n",
    "    lines_bed = []\n",
    "    with open(bed_file, 'r') as f:\n",
    "        lines_bed = f.readlines()\n",
    "\n",
    "    fail_list = []\n",
    "    with open(fasta_fail, 'r') as f:\n",
    "        for line in f:\n",
    "            if re.match(\"chr[0-9XYM]+:[0-9]+\\-[0-9]+\", line):\n",
    "                fail_list.append(line.strip())\n",
    "\n",
    "    lines_fasta = []\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        lines_fasta = f.readlines()\n",
    "\n",
    "    lines_fasta_new = []\n",
    "    idx_bed = 0\n",
    "    flg = 0\n",
    "    for i, line in enumerate(lines_fasta):\n",
    "        if \">chr\" in line and flg == 0:\n",
    "            flg = 1\n",
    "            bed_info = re.findall(\"chr[0-9XYM]+:[0-9]+\\-[0-9]+\", line)[0]\n",
    "            bind_info = re.findall(\"\\([\\-\\+]\\) rbp_bound: [0-1]\", line)[0]\n",
    "            #if i < 20:\n",
    "            #    print(bed_info)\n",
    "            #    print(bind_info)\n",
    "            if bed_info in fail_list:\n",
    "                flg = 0\n",
    "            else:\n",
    "                #if not re.findall(\"chr[0-9XYM]+\", bed_info)[0] == re.findall(\"chr[0-9XYM]+\", lines_bed[idx_bed])[0]:\n",
    "                #    print(bed_info, re.findall(\"chr[0-9XYM]+\", bed_info)[0])\n",
    "                #    print(idx_bed, lines_bed[idx_bed], re.findall(\"chr[0-9XYM]+\", lines_bed[idx_bed])[0])\n",
    "                #assert re.findall(\"chr[0-9XYM]+\", bed_info)[0] == re.findall(\"chr[0-9XYM]+\", lines_bed[idx_bed])[0]\n",
    "                new_line = \">\" + lines_bed[idx_bed].strip() + bind_info\n",
    "                lines_fasta_new.append(new_line)\n",
    "                idx_bed += 1\n",
    "        elif flg==1:\n",
    "            lines_fasta_new.append(line.strip())\n",
    "            flg = 0\n",
    "\n",
    "    with open(fasta_out_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines_fasta_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-surgery",
   "metadata": {},
   "source": [
    "### 5. Create fasta from dev and original_hg38.fasta\n",
    "convert dev.tsv file in the nontraining_finetune directory to the fasta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_DIR = \"./datasets\"\n",
    "SUBDIR1 = \"nontraining_sample_finetune\"\n",
    "TSV_FILE = \"dev.tsv\"\n",
    "FASTA_OUTPUT_FILE = \"dev.fasta\"\n",
    "FASTA_REFERENCE_FILE = \"original_hg38.fasta\"\n",
    "\n",
    "RBPS = (\"AARS\",)\n",
    "for rbp in RBPS:\n",
    "#rbp = \"HNRNPA1\"\n",
    "\n",
    "    fasta_output_file = os.path.join(MASTER_DIR, rbp, SUBDIR1)\n",
    "    if not os.path.isdier(fasta_output_file):\n",
    "        os.makedirs(fasta_output_file)\n",
    "    fasta_output_file = os.path.join(MASTER_DIR, rbp, SUBDIR1, FASTA_OUTPUT_FILE)\n",
    "    tsv_orig_file = os.path.join(MASTER_DIR, rbp, SUBDIR1, TSV_FILE)\n",
    "    fasta_ref_file = os.path.join(MASTER_DIR, rbp, FASTA_REFERENCE_FILE)\n",
    "\n",
    "    df_seqs = pd.read_csv(tsv_orig_file, sep=\"\\t\")\n",
    "    print(\"len(df_seqs)\", len(df_seqs))\n",
    "\n",
    "    lines_fasta_header = []\n",
    "    lines_fasta_seq = []\n",
    "    with open(fasta_ref_file, 'r') as f:\n",
    "        flg = 0\n",
    "        for line in f:\n",
    "            if \">chr\" in line:\n",
    "                chromosome = re.findall(\"chr[0-9XYM]+:\", line)\n",
    "                if chromosome:\n",
    "                    lines_fasta_header.append(line.strip())\n",
    "                    flg = 1\n",
    "                else:\n",
    "                    flg = 0\n",
    "            elif re.match(\"[ATGC]+\", line) and flg==1:\n",
    "                lines_fasta_seq.append(line.strip())\n",
    "                flg = 0\n",
    "            elif re.match(\"[ATGC]+\", line) and flg==0:\n",
    "                flg = 0\n",
    "            else:\n",
    "                print(line)\n",
    "\n",
    "    assert len(lines_fasta_header) == len(lines_fasta_seq)\n",
    "\n",
    "    fasta_new = []\n",
    "    keep_track = int(len(df_seqs)*0.2)\n",
    "    for i in range(len(df_seqs)):\n",
    "        seq = df_seqs[\"sequence\"][i]\n",
    "        seq = kmer2seq(seq)\n",
    "        if seq in lines_fasta_seq[i:]:\n",
    "            idx = i + lines_fasta_seq[i:].index(seq)\n",
    "            fasta_new.append(lines_fasta_header[idx])\n",
    "            fasta_new.append(lines_fasta_seq[idx])\n",
    "        if (i%keep_track==0 and not i==0) or i==len(df_seqs)-1:\n",
    "            print(\"finished converting {}/{} sequences\".format(keep_track, len(df_seqs)))\n",
    "\n",
    "    print(\"len(fasta_new)\", len(fasta_new))\n",
    "\n",
    "    with open(fasta_output_file, 'w') as f:\n",
    "        f.write(\"\\n\".join(fasta_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-durham",
   "metadata": {},
   "source": [
    "### 6. Download transcript annotations\n",
    "6-1. download transcript annotations from the Ensembl website (https://www.ensembl.org/biomart/martview/41609dbda8c4191fad2eab93679401f8)\n",
    "\n",
    "    6-1-1. Choose \"Ensemble Gene 103\" and \"Human genes (GRCh38.p13)\"\n",
    "    6-1-2. In \"Filters/REGION\", select the chromosome 1\n",
    "    6-1-3. In \"Attributes\", select \"Features\", and then check the following boxes in the same order:\n",
    "        \"Gene stable ID\", \"Transcript stable ID\", \"APPRIS annotation\",\n",
    "        \"Transcript support level(TSL)\", \"Transcript length (including UTRs and CDS)\"\n",
    "    6-1-4. Download the result and save it as \"chr1_info.txt\"\n",
    "    6-1-5. In \"Attributes\", select \"Structures\", and then check the following boxes in the same order:\n",
    "        \"Gene stable ID\", \"Transcript stable ID\", \"Transcript start (bp)\",  \"Transcript end (bp)\",\n",
    "         \"5'UTR start\",  \"5'UTR end\",  \"3'UTR start\",  \"3'UTR end\",  \"Exon region start (bp)\",  \"Exon region end (bp)\",\n",
    "    6-1-6. Download the result and save it as \"chr1_value.txt\"\n",
    "    6-1-7. In \"Attributes\", select \"Sequences\", and then check the following boxes in the same order:\n",
    "        \"Unspliced (Transcript)\", \"Gene stable ID\", \"Transcript stable ID\"\n",
    "    6-1-8. Download the results and save it as \"chr1_seq.txt\"\n",
    "    6-1-9. Repeat 6-1-2 through 6-1-8 for chromosome 2-22, MT, X, and Y\n",
    "\n",
    "6-2. convert the information into chrOO.csv by running the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "\n",
    "def load_and_format_chr_info(info_path):\n",
    "    df = pd.read_csv(info_path)\n",
    "\n",
    "    # assign APPRIS rank to each transcript\n",
    "    tmp_array = np.array(df['APPRIS annotation'])\n",
    "    new_array = []\n",
    "    for appris in tmp_array:\n",
    "        if type(appris) == str:\n",
    "            if re.fullmatch('principal[0-9]+', appris):\n",
    "                new_array.append(int(re.search('[0-9]+', appris).group()))\n",
    "            elif re.fullmatch('alternative[0-9]+', appris):\n",
    "                new_array.append(100 + int(re.search('[0-9]+', appris).group()))\n",
    "            else:\n",
    "                print(appris)\n",
    "                raise Exception(\"str type object did not match with APPRIS format\")\n",
    "        elif np.isnan(appris):\n",
    "            new_array.append(appris)\n",
    "        else:\n",
    "            print(appris)\n",
    "            raise Exception(\"found exception for APPRIS annotation\")\n",
    "    # print(len(df), len(tmp_array), len(new_array))\n",
    "    assert len(tmp_array)==len(new_array)\n",
    "    df['APPRIS rank'] = new_array\n",
    "    \n",
    "    # assign TSL rank to each transcript\n",
    "    tmp_array = np.array(df['Transcript support level (TSL)'])\n",
    "    new_array = []\n",
    "    new_array2 = []\n",
    "    for tsl in tmp_array:\n",
    "        if type(tsl) == str:\n",
    "            if 'tslNA' in tsl:\n",
    "                new_array.append('tslNA')\n",
    "                new_array2.append(np.nan)\n",
    "            elif re.fullmatch('tsl[0-9]+', tsl):\n",
    "                tsl_level = re.match('tsl[0-9]+', tsl).group()\n",
    "                new_array.append(tsl_level)\n",
    "                new_array2.append(0)\n",
    "            elif re.fullmatch('tsl[0-9]+ \\(assigned to previous version [0-9]+\\)', tsl):\n",
    "                tsl_level = re.match('tsl[0-9]+', tsl).group()\n",
    "                new_array.append(tsl_level)\n",
    "                prev_version = re.search('version [0-9]+', tsl).group()\n",
    "                prev_version = int(re.search('[0-9]+', prev_version).group())\n",
    "                new_array2.append(prev_version)\n",
    "            else:\n",
    "                print(tsl)\n",
    "                raise Exception(\"str type object did not match with TSL format\")\n",
    "        elif np.isnan(tsl):\n",
    "            new_array.append(tsl)\n",
    "            new_array2.append(np.nan)\n",
    "        else:\n",
    "            print(appris)\n",
    "            raise Exception(\"found exception for TSL\")\n",
    "    # print(len(df), len(tmp_array), len(new_array))\n",
    "    assert len(tmp_array)==len(new_array)\n",
    "    df['TSL rank'] = new_array\n",
    "    df['TSL version'] = new_array2\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ee81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOT_DIR = 'DIRECTORY_WHERE_THE_ANNOTATION_FILES_EXIST'\n",
    "CHROMOSOMES = ['chrX', 'chrY', 'chrM']\n",
    "nums = np.arange(1,22)\n",
    "for num in nums:\n",
    "    CHROMOSOMES.append('chr{}'.format(num))\n",
    "print(CHROMOSOMES)\n",
    "\n",
    "COLUMN_NAMES = [\"Gene_stable_ID\", \"Transcript_stable_ID\", \"Transcript_sequence\",\n",
    "           \"Transcript_start_(bp)\", \"Transcript_end_(bp)\",\n",
    "           \"5'_UTR_start\", \"5'_UTR_end\", \"3'_UTR_start\", \"3'_UTR_end\",\n",
    "           \"Exon_region_start_(bp)\", \"Exon_region_end_(bp)\", \"Strand\"]\n",
    "\n",
    "for chromosome in CHROMOSOMES:\n",
    "    print('processing {} ...'.format(chromosome))\n",
    "    \n",
    "    # read seq data\n",
    "    seq_path = os.path.join(ANNOT_DIR, '{}_seq.txt'.format(chromosome))\n",
    "    seq_data = []\n",
    "    for record in SeqIO.parse(seq_path, 'fasta'):\n",
    "        tmp_data = record.id.split('|')\n",
    "        tmp_data.append(str(record.seq))\n",
    "        seq_data.append(tmp_data)\n",
    "    df_seq = pd.DataFrame(seq_data, columns=['Gene_stable_ID','Transcript_stable_ID','Transcript_sequence'])\n",
    "    print('\\tnumber of transcripts in seq.txt:', len(df_seq))\n",
    "    \n",
    "    # read value data\n",
    "    df_value = pd.read_csv(os.path.join(ANNOT_DIR, '{}_value.txt'.format(chromosome)))\n",
    "    \n",
    "    # read info data\n",
    "    df_info = load_and_format_chr_info(os.path.join(ANNOT_DIR, '{}_info.txt'.format(chromosome)))\n",
    "    unique_gene_ids = np.unique(np.array(df_info['Gene stable ID']))\n",
    "    print('\\tnumber of transcripts in info.txt:', len(df_info))\n",
    "    print('\\tnumber of unique gene ids in info.txt:', len(unique_gene_ids))\n",
    "    save_info = []\n",
    "    for gene_id in unique_gene_ids:\n",
    "        tmp_save = []\n",
    "        # record gene id\n",
    "        tmp_save.append(gene_id)\n",
    "        \n",
    "        # record the best transcript\n",
    "        df_tmp = df_info[df_info['Gene stable ID']==gene_id].copy()\n",
    "        df_tmp = df_tmp.sort_values(['APPRIS rank', 'TSL rank', 'Transcript length (including UTRs and CDS)', 'TSL version'], ascending=[True, True, False, False])\n",
    "        transcript_id = df_tmp.iloc[0,1]\n",
    "        tmp_save.append(transcript_id)\n",
    "        \n",
    "        # record sequence\n",
    "        sequence = df_seq.query('Gene_stable_ID==\"{}\" & Transcript_stable_ID==\"{}\"'.format(gene_id, transcript_id))['Transcript_sequence'].values[0]\n",
    "        assert type(sequence)==str\n",
    "        assert re.fullmatch('[ATGCN]+', sequence)\n",
    "        tmp_save.append(sequence)\n",
    "        \n",
    "        df_tmp = df_value[df_value['Transcript stable ID']==transcript_id]\n",
    "        # record transcript info\n",
    "        ts_start = np.unique(df_tmp['Transcript start (bp)'].values).astype(int)\n",
    "        ts_end = np.unique(df_tmp['Transcript end (bp)'].values).astype(int)\n",
    "        if not len(ts_start)==1:\n",
    "            print(gene_id, transcript_id)\n",
    "        assert len(ts_start)==1\n",
    "        assert len(ts_end)==1\n",
    "        tmp_save.append(ts_start[0])\n",
    "        tmp_save.append(ts_end[0])\n",
    "        \n",
    "        # record regiontype info\n",
    "        for tmp_column in [\"5' UTR start\", \"5' UTR end\", \"3' UTR start\", \"3' UTR end\", \"Exon region start (bp)\", \"Exon region end (bp)\"]:\n",
    "            tmp_array = np.array(df_tmp[tmp_column])\n",
    "            tmp_array = list(tmp_array[~np.isnan(tmp_array)].astype(int))\n",
    "            tmp_save.append(tmp_array)\n",
    "        assert len(tmp_save[-1])==len(tmp_save[-2])\n",
    "        assert len(tmp_save[-3])==len(tmp_save[-4])\n",
    "        assert len(tmp_save[-5])==len(tmp_save[-6])\n",
    "\n",
    "        # record strand info\n",
    "        strand = np.unique(df_tmp['Strand'].values).astype(int)\n",
    "        assert len(strand)==1\n",
    "        tmp_save.append(strand[0])\n",
    "        save_info.append(tmp_save)\n",
    "\n",
    "    df_save = pd.DataFrame(save_info, columns=COLUMN_NAMES)\n",
    "    save_path = os.path.join(ANNOT_DIR, '{}.csv'.format(chromosome))\n",
    "    df_save.to_csv(save_path, index=False)\n",
    "    print('\\tsaved into {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-singer",
   "metadata": {},
   "source": [
    "### 7. Generate annotation.npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "appointed-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append('./motif')\n",
    "from motif_utils import seq2kmer, kmer2seq\n",
    "from Bio import Align\n",
    "from Bio.Seq import reverse_complement\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "# this takes 1-based genomic positions (overlap_start, overlap_end, region_start, region_end)\n",
    "# this returns 0-based indeces within the overlap sequence (index 0-100 for 101-length overlap sequence)\n",
    "def assign_region(overlap_start, overlap_end, region_start, region_end, seq):\n",
    "    assigned_start = -1\n",
    "    assigned_end = 0\n",
    "    if overlap_start > region_end  or overlap_end < region_start:\n",
    "        pass\n",
    "    else:\n",
    "        if overlap_start >= region_start:\n",
    "            assigned_start = 0\n",
    "        else:\n",
    "            assigned_start = region_start - overlap_start\n",
    "        if overlap_end <= region_end:\n",
    "            assigned_end = len(seq)\n",
    "        else:\n",
    "            assigned_end = region_end - overlap_start + 1\n",
    "    return assigned_start, assigned_end\n",
    "\n",
    "# this takes 1-based genomic positions (transcript_start, transcript_end, query_start, query_end)\n",
    "# this returns 1-based genomic positions\n",
    "def assign_transcript(transcript_start, transcript_end, query_start, query_end):\n",
    "    transcript_sub_start = 0\n",
    "    transcript_sub_end = 0\n",
    "    if transcript_start > query_end  or transcript_end < query_start:\n",
    "        assert False\n",
    "    else:\n",
    "        if transcript_start >= query_start:\n",
    "            transcript_sub_start = transcript_start\n",
    "        else:\n",
    "            transcript_sub_start = query_start\n",
    "        if transcript_end <= query_end:\n",
    "            transcript_sub_end = transcript_end\n",
    "        else:\n",
    "            transcript_sub_end = query_end\n",
    "            \n",
    "    return transcript_sub_start, transcript_sub_end\n",
    "\n",
    "def get_annotation(annot_dir, chromosome, query_start, query_end, strand, query_seq):\n",
    "    path = os.path.join(annot_dir, (chromosome + \".csv\"))\n",
    "    df_chr = pd.read_csv(path)\n",
    "    allow_mismatch = 0.05 #ratio\n",
    "    thresh_for_partial = 0.5\n",
    "    \n",
    "    # 5UTR, 3UTR, exon, intron, CDS\n",
    "    annotations = np.zeros([5, len(query_seq)])\n",
    "    for i in range(len(df_chr)):\n",
    "        transcript_start = int(df_chr[\"Transcript_start_(bp)\"][i])\n",
    "        transcript_end = int(df_chr[\"Transcript_end_(bp)\"][i])\n",
    "        if query_start > transcript_end or query_end < transcript_start: # no overlap\n",
    "            pass\n",
    "        else:\n",
    "            # retrieve sequences\n",
    "            transcript_seq = df_chr[\"Transcript_sequence\"][i]\n",
    "            if df_chr[\"Strand\"][i] == -1:\n",
    "                transcript_seq = reverse_complement(transcript_seq)\n",
    "            if strand == \"-\":\n",
    "                query_seq = reverse_complement(query_seq)\n",
    "                annotations = np.flip(annotations, axis=1)\n",
    "            tmp_start, tmp_end = assign_region(transcript_start, transcript_end, query_start, query_end, transcript_seq)\n",
    "            transcript_seq = transcript_seq[tmp_start:tmp_end]\n",
    "            transcript_sub_start, transcript_sub_end = assign_transcript(transcript_start, transcript_end, query_start, query_end)\n",
    "            \n",
    "            # retrieve 5'UTR positions\n",
    "            utr5_starts = re.findall(\"[0-9]+\", df_chr[\"5'_UTR_start\"][i])\n",
    "            utr5_ends = re.findall(\"[0-9]+\", df_chr[\"5'_UTR_end\"][i])\n",
    "            if len(utr5_starts) > 0:\n",
    "                utr5_starts = list(map(int, utr5_starts))\n",
    "            if len(utr5_ends) > 0:\n",
    "                utr5_ends = list(map(int, utr5_ends))\n",
    "            # retrieve 3'UTR positions\n",
    "            utr3_starts = re.findall(\"[0-9]+\", df_chr[\"3'_UTR_start\"][i])\n",
    "            utr3_ends = re.findall(\"[0-9]+\", df_chr[\"3'_UTR_end\"][i])\n",
    "            if len(utr3_starts) > 0:\n",
    "                utr3_starts = list(map(int, utr3_starts))\n",
    "            if len(utr3_ends) > 0:\n",
    "                utr3_ends = list(map(int, utr3_ends))\n",
    "            # retrieve exon positions\n",
    "            exon_starts = re.findall(\"[0-9]+\", df_chr[\"Exon_region_start_(bp)\"][i])\n",
    "            exon_ends = re.findall(\"[0-9]+\", df_chr[\"Exon_region_end_(bp)\"][i])\n",
    "            if len(exon_starts) > 0:\n",
    "                exon_starts = list(map(int, exon_starts))\n",
    "            if len(exon_ends) > 0:\n",
    "                exon_ends = list(map(int, exon_ends))\n",
    "            \n",
    "            if query_start >= transcript_start and query_end <= transcript_end: # perfect overlap\n",
    "                # detect overlapping region\n",
    "                overlap_starts = []\n",
    "                overlap_ends = []\n",
    "                aligner = Align.PairwiseAligner()\n",
    "                aligner.gap_score = -10000\n",
    "                aligner.internal_gap_score = -1\n",
    "                aligner.mode = \"local\"\n",
    "                alignment = aligner.align(transcript_seq, query_seq)[0]\n",
    "                if alignment.score > len(query_seq)*(1-allow_mismatch):\n",
    "                    match_starts = alignment.aligned[0][0]\n",
    "                    start_offsets = alignment.aligned[1][0]\n",
    "                    for match_start, start_offset in zip(match_starts, start_offsets):\n",
    "                        overlap_start = transcript_sub_start + match_start - start_offset\n",
    "                        overlap_starts.append(overlap_start)\n",
    "                else:\n",
    "                    query_seq = reverse_complement(query_seq)\n",
    "                    annotations = np.flip(annotations, axis=1)\n",
    "                    alignment = aligner.align(transcript_seq, query_seq)[0]\n",
    "                    if alignment.score > len(query_seq)*(1-allow_mismatch):\n",
    "                        print(\"query strand(+/-) is wrong\")\n",
    "                        print(\"transcriptID: {}, {}, query_start {}, query_end {}, strand {}, query sequence:\\n {}\".format(\\\n",
    "                                    df_chr[\"Transcript_stable_ID\"][i], chromosome, query_start, query_end, strand, query_seq))\n",
    "                        match_starts = alignment.aligned[0][0]\n",
    "                        start_offsets = alignment.aligned[1][0]\n",
    "                        for match_start, start_offset in zip(match_starts, start_offsets):\n",
    "                            overlap_start = transcript_sub_start + match_start - start_offset\n",
    "                            overlap_starts.append(overlap_start)\n",
    "                    else:\n",
    "                        print(\"ERROR: query overlapped, but no match was found\")\n",
    "                        print(\"transcript #{}, ID: {}, chr {}, query_start {}, query_end {}, strand {}, query sequence:\\n {}\".format(\\\n",
    "                                    i, df_chr[\"Transcript_stable_ID\"][i], chromosome, query_start, query_end, strand, query_seq))\n",
    "                        break\n",
    "                        #assert False\n",
    "                        \n",
    "                for overlap_start in overlap_starts:\n",
    "                    overlap_end = overlap_start + len(query_seq) - 1\n",
    "                    overlap_ends.append(overlap_end)\n",
    "                # print(\"transcriptID: {}, strand: {}\\n{}:{}-{}\".format(df_chr[\"Transcript_stable_ID\"][i], df_chr[\"Strand\"][i], chromosome, overlap_start, overlap_end))\n",
    "                assert overlap_start >=query_start and overlap_end <=query_end\n",
    "\n",
    "                # assign 5'UTR(index=0)\n",
    "                for utr5_start, utr5_end in zip(utr5_starts, utr5_ends):\n",
    "                    for overlap_start, overlap_end in zip(overlap_starts, overlap_ends):\n",
    "                        assigned_start, assigned_end = assign_region(overlap_start, overlap_end, utr5_start, utr5_end, query_seq)\n",
    "                        annotations[0, assigned_start:assigned_end] = 1\n",
    "                # assign 3'UTR(index=1)\n",
    "                for utr3_start, utr3_end in zip(utr3_starts, utr3_ends):\n",
    "                    for overlap_start, overlap_end in zip(overlap_starts, overlap_ends):\n",
    "                        assigned_start, assigned_end = assign_region(overlap_start, overlap_end, utr3_start, utr3_end, query_seq)\n",
    "                        annotations[1, assigned_start:assigned_end] = 1\n",
    "                # assign exon(index=2)\n",
    "                for exon_start, exon_end in zip(exon_starts, exon_ends):\n",
    "                    for overlap_start, overlap_end in zip(overlap_starts, overlap_ends):\n",
    "                        assigned_start, assigned_end = assign_region(overlap_start, overlap_end, exon_start, exon_end, query_seq)\n",
    "                        annotations[2, assigned_start:assigned_end] = 1\n",
    "\n",
    "                # assign cds(index=5) and intron(index=4)\n",
    "                for pos in range(annotations.shape[1]):\n",
    "                    if annotations[2,pos]==1 and np.sum(annotations[0:2, pos])==0:\n",
    "                        annotations[4,pos] = 1\n",
    "                    if np.sum(annotations[:,pos])==0:\n",
    "                        annotations[3,pos] = 1\n",
    "                \n",
    "                if strand == \"-\":\n",
    "                    query_seq = reverse_complement(query_seq)\n",
    "                    annotations = np.flip(annotations, axis=1)\n",
    "                \n",
    "                break\n",
    "            else: # partial overlap\n",
    "                # detect overlapping region\n",
    "                overlap_start = -1\n",
    "                overlap_end = -1\n",
    "                query_match_start = -1\n",
    "                query_match_end = 0\n",
    "                aligner = Align.PairwiseAligner()\n",
    "                aligner.gap_score = -10000\n",
    "                aligner.mode = \"local\"\n",
    "                alignment = aligner.align(transcript_seq, query_seq)[0]\n",
    "                if alignment.score > len(transcript_seq)*thresh_for_partial or alignment.score > 10:\n",
    "                    match_start = alignment.aligned[0][0][0]\n",
    "                    match_end = alignment.aligned[0][0][1]\n",
    "                    query_match_start = alignment.aligned[1][0][0]\n",
    "                    query_match_end = alignment.aligned[1][0][1]\n",
    "                    overlap_start = transcript_sub_start + match_start - query_match_start\n",
    "                    overlap_end = transcript_sub_start + match_end - 1\n",
    "                else:\n",
    "                    query_seq = reverse_complement(query_seq)\n",
    "                    annotations = np.flip(annotations, axis=1)\n",
    "                    alignment = aligner.align(transcript_seq, query_seq)[0]\n",
    "                    if alignment.score > len(transcript_seq)*thresh_for_partial or alignment.score > 10:\n",
    "                        print(\"query strand(+/-) is wrong\")\n",
    "                        print(\"transcriptID: {}, {}, query_start {}, query_end {}, strand {}, query sequence:\\n {}\".format(\\\n",
    "                                    df_chr[\"Transcript_stable_ID\"][i], chromosome, query_start, query_end, strand, query_seq))\n",
    "                        match_start = alignment.aligned[0][0][0]\n",
    "                        match_end = alignment.aligned[0][0][1]\n",
    "                        query_match_start = alignment.aligned[1][0][0]\n",
    "                        query_match_end = alignment.aligned[1][0][1]\n",
    "                        overlap_start = transcript_sub_start + match_start - query_match_start\n",
    "                        overlap_end = transcript_sub_start + match_end - 1\n",
    "                    else:\n",
    "                        if strand == \"+\":\n",
    "                            query_seq = reverse_complement(query_seq)\n",
    "                            annotations = np.flip(annotations, axis=1)\n",
    "                        print(\"alignment score is too low: score {}, len {}\".format(alignment.score, len(transcript_seq)))\n",
    "                        print(\"transcriptID: {}, {}, query_start {}, query_end {}, strand {}, query sequence:\\n {}\".format(\\\n",
    "                                    df_chr[\"Transcript_stable_ID\"][i], chromosome, query_start, query_end, strand, query_seq))\n",
    "                        continue\n",
    "                \n",
    "                # assign 5'UTR(index=0)\n",
    "                for utr5_start, utr5_end in zip(utr5_starts, utr5_ends):\n",
    "                    assigned_start, assigned_end = assign_region(overlap_start, overlap_end, utr5_start, utr5_end, query_seq)\n",
    "                    annotations[0, assigned_start:assigned_end] = 1\n",
    "                # assign 3'UTR(index=1)\n",
    "                for utr3_start, utr3_end in zip(utr3_starts, utr3_ends):\n",
    "                    assigned_start, assigned_end = assign_region(overlap_start, overlap_end, utr3_start, utr3_end, query_seq)\n",
    "                    annotations[1, assigned_start:assigned_end] = 1\n",
    "                # assign exon(index=2)\n",
    "                for exon_start, exon_end in zip(exon_starts, exon_ends):\n",
    "                    assigned_start, assigned_end = assign_region(overlap_start, overlap_end, exon_start, exon_end, query_seq)\n",
    "                    annotations[2, assigned_start:assigned_end] = 1\n",
    "\n",
    "                # assign cds(index=5) and intron(index=4)\n",
    "                for pos in range(query_match_start, query_match_end):\n",
    "                    if annotations[2,pos]==1 and np.sum(annotations[0:2, pos])==0:\n",
    "                        annotations[4,pos] = 1\n",
    "                    if np.sum(annotations[:,pos])==0:\n",
    "                        annotations[3,pos] = 1\n",
    "                \n",
    "                if strand == \"-\":\n",
    "                    query_seq = reverse_complement(query_seq)\n",
    "                    annotations = np.flip(annotations, axis=1)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_DIR = \"./datasets\"\n",
    "ANNOT_DIR = \"DIRECTORY_WHERE_THE_ANNOTATION_FILES_EXIST\"\n",
    "RBPS = (\"AARS\",)\n",
    "for rbp in RBPS:\n",
    "    FASTA_PATH = os.path.join(MASTER_DIR, rbp, \"nontraining_sample_finetune/dev.fasta\")\n",
    "    OUTPUT_PATH = os.path.join(MASTER_DIR, rbp, \"nontraining_sample_finetune/annotation.npy\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--fasta_path\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"path to the hg38 fasta file.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--annotation_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"directory where the reference annotations exist\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_path\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"path to the output npy file.\",\n",
    "    )\n",
    "    args = parser.parse_args(args = [\"--fasta_path\", FASTA_PATH,\\\n",
    "                                                     \"--annotation_dir\", ANNOT_DIR,\\\n",
    "                                                     \"--output_path\",OUTPUT_PATH])\n",
    "\n",
    "    fasta_file, output_file = args.fasta_path, args.output_path\n",
    "    if not \".fasta\" in fasta_file:\n",
    "        fasta_file = os.path.join(args.fasta_path, \"dev.fasta\")\n",
    "    if not \".npy\" in output_file:\n",
    "        output_file = os.path.join(args.output_path, \"annotations.npy\")\n",
    "\n",
    "    lines_fasta_header = []\n",
    "    lines_fasta_seq = []\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        flg = 0\n",
    "        for line in f:\n",
    "            if \">chr\" in line:\n",
    "                lines_fasta_header.append(line.strip())\n",
    "            elif re.match(\"[ATGC]+\", line):\n",
    "                lines_fasta_seq.append(line.strip())\n",
    "            else:\n",
    "                print(line)\n",
    "\n",
    "    assert len(lines_fasta_header) == len(lines_fasta_seq)\n",
    "\n",
    "    annotations_all = np.array([])\n",
    "    keep_track = int(len(lines_fasta_header)*0.2)\n",
    "    print(\"start annotating\")\n",
    "    for i, line in enumerate(lines_fasta_header):\n",
    "        if \">chr\" in line:\n",
    "            chromosome = re.findall(\"chr[0-9XYM]+:\", line)\n",
    "            if chromosome:\n",
    "                chromosome = chromosome[0][:-1]\n",
    "                query_start = int(re.findall(\"[0-9]+\\-\", line)[0][:-1])\n",
    "                query_end = int(re.findall(\"\\-[0-9]+\", line)[0][1:])\n",
    "                strand = re.findall(\"\\([\\-\\+]\\)\", line)[0][1]\n",
    "                rbp_bound = re.findall(\"rbp_bound: [0-1]\", line)[0][-1]\n",
    "                query_seq = lines_fasta_seq[i]\n",
    "\n",
    "                annotations = get_annotation(args.annotation_dir, chromosome, query_start, query_end, strand, query_seq)\n",
    "                annotations = annotations[np.newaxis, :, :]\n",
    "                if len(annotations_all)==0:\n",
    "                    annotations_all = annotations\n",
    "                else:\n",
    "                    annotations_all = np.concatenate([annotations_all, annotations], axis=0)\n",
    "                if (i%keep_track==0 and not i==0) or i==len(lines_fasta_header)-1:\n",
    "                    print(\"finished annotating {}/{} sequences\".format(keep_track, len(lines_fasta_header)))\n",
    "\n",
    "    np.save(output_file, annotations_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
